{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de Integração Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuração dos acessos a chave segura, conexões, leitura e escrita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipywidgets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mwidgets\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultAzureCredential\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Configuração do Azure Key Vault\n",
    "key_vault_name = \"SEU_KEY_VAULT_NAME\"\n",
    "key_vault_uri = f\"https://{key_vault_name}.vault.azure.net/\"\n",
    "credential = DefaultAzureCredential()\n",
    "client = SecretClient(vault_url=key_vault_uri, credential=credential)\n",
    "\n",
    "# Função para recuperar secret do Azure Key Vault\n",
    "def get_secret(secret_name):\n",
    "    retrieved_secret = client.get_secret(secret_name)\n",
    "    return retrieved_secret.value\n",
    "\n",
    "# Função de leitura de dados do PostgreSQL\n",
    "def ler_dados_postgresql(senha):\n",
    "    usuario = get_secret(\"postgres-usuario\")\n",
    "    host = get_secret(\"postgres-host\")\n",
    "    db = get_secret(\"postgres-db\")\n",
    "    tabela = \"nome_tabela\" \n",
    "    url = f\"postgresql://{usuario}:{senha}@{host}/{db}\"\n",
    "    engine = create_engine(url)\n",
    "    df = pd.read_sql_table(tabela, con=engine)\n",
    "    return df\n",
    "\n",
    "def ler_dados_lake():\n",
    "    caminho_arquivo = get_secret(\"lake-caminho-arquivo\")\n",
    "    df = pd.read_parquet(caminho_arquivo)\n",
    "    return df\n",
    "\n",
    "# Widgets para interação e leitura de dados\n",
    "selecao_origem = widgets.Dropdown(\n",
    "    options=['PostgreSQL', 'Data Lake'],\n",
    "    value='PostgreSQL',\n",
    "    description='Origem:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "botao_carregar = widgets.Button(description=\"Carregar Dados\")\n",
    "\n",
    "# Função chamada quando o botão é clicado\n",
    "def on_carregar_clicked(b):\n",
    "    origem = selecao_origem.value\n",
    "    if origem == 'PostgreSQL':\n",
    "        # Recupera a senha do banco de dados do Azure Key Vault\n",
    "        senha = get_secret(\"postgres-senha\")\n",
    "        df = ler_dados_postgresql(senha)\n",
    "    else:\n",
    "        df = ler_dados_lake()\n",
    "    display(df.head())\n",
    "\n",
    "botao_carregar.on_click(on_carregar_clicked)\n",
    "\n",
    "# Exibir widgets\n",
    "display(selecao_origem, botao_carregar)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set do ambiente para recebimento do arquivo de atualização da tabela com Azure Functions e Logic Apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.functions as func\n",
    "import json\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "def main(req: func.HttpRequest) -> func.HttpResponse:\n",
    "    try:\n",
    "        # Parâmetros 'fidedignos' de conexão ao Azure Blob Storage\n",
    "        connect_str = \"DefaultEndpointsProtocol=https;AccountName=SEU_ACCOUNT_NAME;AccountKey=SEU_ACCOUNT_KEY;EndpointSuffix=core.windows.net\"\n",
    "        container_name = \"SEU_CONTAINER_NAME\"\n",
    "        \n",
    "        # Inicializa o cliente do Blob Service\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "        \n",
    "        # Recupera o corpo da requisição\n",
    "        dados = req.get_json()\n",
    "        dados_str = json.dumps(dados)\n",
    "        \n",
    "        # Nome do blob pode ser uma combinação de ID único ou timestamp\n",
    "        blob_name = f\"dados_coletados_{dados['id']}.json\"\n",
    "        \n",
    "        # Cria um blob e carrega os dados\n",
    "        blob_client = container_client.get_blob_client(blob_name)\n",
    "        blob_client.upload_blob(dados_str, overwrite=True)\n",
    "        \n",
    "        return func.HttpResponse(f\"Dados armazenados com sucesso no blob: {blob_name}\", status_code=200)\n",
    "    except Exception as e:\n",
    "        return func.HttpResponse(f\"Erro ao processar a requisição: {str(e)}\", status_code=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento dos dados de Entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, to_timestamp\n",
    "\n",
    "# Inicialização da sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Tratamento de Dados Loans e Clients\").getOrCreate()\n",
    "\n",
    "# Configuração do acesso ao Azure Blob Storage (ajuste conforme necessário)\n",
    "spark.conf.set(\"fs.azure.account.key.SEU_ACCOUNT_NAME.blob.core.windows.net\", \"SEU_ACCOUNT_KEY\")\n",
    "\n",
    "# Carregamento dos dados coletados do Azure Blob Storage\n",
    "dados_df = spark.read.json(\"dbfs:/mnt/SEU_CAMINHO_PARA_O_BLOB/dados_coletados_*.json\")\n",
    "\n",
    "# Tratamento dos dados para a tabela 'clients'\n",
    "clients_df = dados_df.select(\n",
    "    col(\"user_id\").cast(\"integer\"),\n",
    "    to_timestamp(col(\"created_at\"), \"yyyy-MM-dd'T'HH:mm:ss\").alias(\"created_at\"),\n",
    "    col(\"status\"),\n",
    "    col(\"batch\").cast(\"integer\"),\n",
    "    col(\"credit_limit\").cast(\"decimal(10,2)\"),\n",
    "    col(\"interest_rate\").cast(\"decimal(10,2)\")\n",
    ").distinct()  # Removendo duplicatas, se houver\n",
    "\n",
    "# Tratamento dos dados para a tabela 'loans'\n",
    "loans_df = dados_df.select(\n",
    "    col(\"user_id\").cast(\"integer\"),\n",
    "    col(\"loan_id\").cast(\"integer\"),\n",
    "    to_date(col(\"loan_created_at\"), \"yyyy-MM-dd\").alias(\"created_at\"),\n",
    "    to_date(col(\"due_at\"), \"yyyy-MM-dd\").alias(\"due_at\"),\n",
    "    to_date(col(\"paid_at\"), \"yyyy-MM-dd\").alias(\"paid_at\"),\n",
    "    col(\"status\"),\n",
    "    col(\"loan_amount\").cast(\"decimal(10,2)\"),\n",
    "    col(\"tax\").cast(\"decimal(10,2)\"),\n",
    "    col(\"due_amount\").cast(\"decimal(10,2)\"),\n",
    "    col(\"amount_paid\").cast(\"decimal(10,2)\")\n",
    ").distinct()  # Removendo duplicatas, se houver\n",
    "\n",
    "# Salvar os DataFrames tratados em um local temporário ou prosseguir com o upsert diretamente\n",
    "clients_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/clients_temp\")\n",
    "loans_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/loans_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge dos Dados e Atualização no Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdelta\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeltaTable\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Caminhos para as tabelas Delta existentes\u001b[39;00m\n\u001b[0;32m      3\u001b[0m path_clients_delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/delta/clients\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'delta'"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "# Caminhos para as tabelas Delta existentes\n",
    "path_clients_delta = \"/mnt/delta/clients\"\n",
    "path_loans_delta = \"/mnt/delta/loans\"\n",
    "\n",
    "# Carregar os DataFrames tratados do diretório temporário\n",
    "df_clients_tratado = spark.read.format(\"delta\").load(\"/mnt/delta/clients_temp\")\n",
    "df_loans_tratado = spark.read.format(\"delta\").load(\"/mnt/delta/loans_temp\")\n",
    "\n",
    "# Realizar upsert na tabela 'clients'\n",
    "deltaTable_clients = DeltaTable.forPath(spark, path_clients_delta)\n",
    "\n",
    "(deltaTable_clients.alias(\"existing\")\n",
    " .merge(\n",
    "     df_clients_tratado.alias(\"updates\"),\n",
    "     \"existing.user_id = updates.user_id\")\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute()\n",
    ")\n",
    "\n",
    "# Realizar upsert na tabela 'loans'\n",
    "deltaTable_loans = DeltaTable.forPath(spark, path_loans_delta)\n",
    "\n",
    "(deltaTable_loans.alias(\"existing\")\n",
    " .merge(\n",
    "     df_loans_tratado.alias(\"updates\"),\n",
    "     \"existing.loan_id = updates.loan_id\")\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise dos Dados consolidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Carregamento dos dados\n",
    "path_clients = '/mnt/data/clients.csv'\n",
    "path_loans = '/mnt/data/loans.csv'\n",
    "clients_df = pd.read_csv(path_clients)\n",
    "loans_df = pd.read_csv(path_loans)\n",
    "\n",
    "# Análise 1: Distribuição do Limite de Crédito dos Clientes Aprovados\n",
    "clientes_aprovados = clients_df[clients_df['status'] == 'approved']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(clientes_aprovados['credit_limit'], bins=50, kde=True, color='blue')\n",
    "plt.title('Distribuição do Limite de Crédito dos Clientes Aprovados')\n",
    "plt.xlabel('Limite de Crédito')\n",
    "plt.ylabel('Frequência')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Análise 2: Taxa de Inadimplência Geral\n",
    "total_emprestimos = len(loans_df)\n",
    "emprestimos_inadimplentes = len(loans_df[loans_df['status'] == 'default'])\n",
    "taxa_inadimplencia_geral = emprestimos_inadimplentes / total_emprestimos\n",
    "print(f\"Taxa de Inadimplência Geral: {taxa_inadimplencia_geral:.2%}\")\n",
    "\n",
    "# Análise 3: Total de Empréstimos por Status\n",
    "total_emprestimos_por_status = loans_df['status'].value_counts().reset_index()\n",
    "total_emprestimos_por_status.columns = ['status', 'total']\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=total_emprestimos_por_status, x='status', y='total', palette='coolwarm')\n",
    "plt.title('Total de Empréstimos por Status')\n",
    "plt.xlabel('Status do Empréstimo')\n",
    "plt.ylabel('Total de Empréstimos')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Análise 4: Taxa de Inadimplência por Batch\n",
    "emprestimos_por_status_batch = loans_df.merge(clients_df[['user_id', 'batch']], on='user_id', how='left')\n",
    "emprestimos_por_status_batch = emprestimos_por_status_batch.groupby(['batch', 'status']).size().reset_index(name='total')\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.barplot(data=emprestimos_por_status_batch, x='batch', y='total', hue='status', palette='Set2')\n",
    "plt.title('Total de Empréstimos por Status em Cada Batch')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Total de Empréstimos')\n",
    "plt.grid(axis='y')\n",
    "plt.legend(title='Status do Empréstimo')\n",
    "plt.show()\n",
    "\n",
    "# Análise 5: Risco Absoluto por Batch\n",
    "loans_with_batch_info = loans_df.merge(clients_df[['user_id', 'batch']], on='user_id', how='left')\n",
    "risco_absoluto_por_batch = loans_with_batch_info[loans_with_batch_info['status'] == 'default'].groupby('batch')['due_amount'].sum().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=risco_absoluto_por_batch, x='batch', y='due_amount', palette='rocket')\n",
    "plt.title('Risco Absoluto por Batch')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Soma do Valor Devido por Empréstimos Inadimplentes')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previsão de Inadimplência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de preparação dos dados para a previsão de inadimplência\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Supondo que 'loans_df' já tenha uma coluna 'default_flag' indicando inadimplência (1 para default, 0 para não default)\n",
    "features = ['credit_limit', 'interest_rate', 'loan_amount', 'tax']  # Exemplo de features\n",
    "X = loans_df[features]\n",
    "y = loans_df['default_flag']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Construção do modelo\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Avaliação do modelo\n",
    "predictions = model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Seleção de variáveis para clusterização e normalização\n",
    "X_cluster = StandardScaler().fit_transform(clients_df[['credit_limit', 'interest_rate']])\n",
    "\n",
    "# Construção do modelo de clusterização\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_cluster)\n",
    "\n",
    "# Adicionando a informação de cluster ao DataFrame original\n",
    "clients_df['cluster'] = clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Selecionando features para a clusterização\n",
    "features_segmentacao = clients_df[['credit_limit', 'interest_rate']].values\n",
    "\n",
    "# Normalização das features\n",
    "scaler = StandardScaler()\n",
    "features_segmentacao_scaled = scaler.fit_transform(features_segmentacao)\n",
    "\n",
    "# Definindo o número de clusters\n",
    "k = 3\n",
    "\n",
    "# Realizando a clusterização\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "clients_df['cluster'] = kmeans.fit_predict(features_segmentacao_scaled)\n",
    "\n",
    "# Visualização da Segmentação de Clientes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=clients_df, x='credit_limit', y='interest_rate', hue='cluster', palette='Set1')\n",
    "plt.title('Segmentação de Clientes por Limite de Crédito e Taxa de Juros')\n",
    "plt.xlabel('Limite de Crédito')\n",
    "plt.ylabel('Taxa de Juros')\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adição e Alteração de usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# Conexão com o banco de dados\n",
    "engine = sqlalchemy.create_engine('postgresql://user:password@host:port/database')\n",
    "\n",
    "def adicionar_usuario(engine, novo_usuario):\n",
    "    novo_usuario_df = pd.DataFrame([novo_usuario])\n",
    "    novo_usuario_df.to_sql('clients', con=engine, if_exists='append', index=False)\n",
    "\n",
    "def alterar_usuario(engine, user_id, atualizacoes):\n",
    "    with engine.connect() as con:\n",
    "        for coluna, valor in atualizacoes.items():\n",
    "            query = sqlalchemy.text(f\"UPDATE clients SET {coluna} = :valor WHERE user_id = :user_id\")\n",
    "            con.execute(query, valor=valor, user_id=user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Configurando o logging\n",
    "logging.basicConfig(filename='log_diario.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "\n",
    "# Exemplo de função para simular uma tarefa diária\n",
    "def tarefa_diaria():\n",
    "    # Simulação de tarefa\n",
    "    sucesso = True  # Simulação de sucesso da tarefa\n",
    "    if sucesso:\n",
    "        logging.info(\"Tarefa diária executada com sucesso.\")\n",
    "    else:\n",
    "        logging.error(\"Falha na execução da tarefa diária.\")\n",
    "\n",
    "tarefa_diaria()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
